{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "PROGETTO4_classification_embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWhBTkmnVdey"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nlwZ-HsWlVU",
        "outputId": "d03a5796-dab7-47f8-dea2-8677338a53a6"
      },
      "source": [
        "# Install PyDrive\n",
        "!pip install PyDrive\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "#Import modules\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Get the Shareable link\n",
        "#Ex BIG : https://drive.google.com/file/d/1aWNJjMp9Ks-ponG8MLpHz6eRYS-gzA4i/view?usp=sharing\n",
        "#https://drive.google.com/file/d/143Je73S7FHWusXN2n-gYq86BWtFwufu8/view?usp=sharing\n",
        "\n",
        "#ORIGINALE\n",
        "#https://drive.google.com/file/d/1QEaoJiiCvakAo-zf79gjhK0Aw9GbsdvG/view?usp=sharing\n",
        "downloaded = drive.CreateFile({'id':\"143Je73S7FHWusXN2n-gYq86BWtFwufu8\"})   \n",
        "downloaded.GetContentFile('ds_train1.csv')       \n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (50.3.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2nWI-P-Wzwg"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"ds_train1.csv\",encoding  = \"UTF-8\")\n",
        "df_not_na = df[~(df['abstract_cleaned'].isna())]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOpfDrN7giCt"
      },
      "source": [
        "abstract_0 = df_not_na['abstract_cleaned']\n",
        "abstract = abstract_0.apply(lambda x: x.split(\",\"))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "8CEV2eQ5XsPl",
        "outputId": "61abcc8e-770c-4ca1-910d-534bb2e407d7"
      },
      "source": [
        "df_not_na['abstract_cleaned'] = abstract\n",
        "df_not_na['Keywords'] = df_not_na['Keywords'].apply(lambda x: x.split(\",\"))\n",
        "df = df_not_na.explode('Keywords')\n",
        "df[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>abstract</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>text</th>\n",
              "      <th>abstract_spacy</th>\n",
              "      <th>abstract_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytic_provenance</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>applied_machine_learning</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>user_interaction</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>flow_visualization</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>adolescent</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>adult</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>computer_graphics</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>computer_interface</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>decision_making</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>image_processing</td>\n",
              "      <td>visual,analytics,inherently,collaboration,curr...</td>\n",
              "      <td>analytics collaboration analytics interaction ...</td>\n",
              "      <td>[analytic_provenance, applied_machine_learning...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  ...                                   abstract_cleaned\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "0  2014  ...  [analytic_provenance, applied_machine_learning...\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZXrEjFJaAHL"
      },
      "source": [
        "df_mf = df.groupby('Keywords').filter(lambda x: x['Keywords'].count()>10)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDK9HMexHGNt",
        "outputId": "f70f2cf2-ae35-42cf-fc72-d59b3a76e983"
      },
      "source": [
        "keywords = df['Keywords'].value_counts()[:30]\n",
        "keywords"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "learning_systems                   8587\n",
              "artificial_intelligence            5873\n",
              "machine_learning                   5155\n",
              "learning_algorithms                3916\n",
              "support_vector_machines            2854\n",
              "classification_(of_information)    2514\n",
              "data_mining                        1927\n",
              "neural_networks                    1625\n",
              "feature_extraction                 1532\n",
              "support_vector_machine             1399\n",
              "extreme_learning_machine           1389\n",
              "decision_trees                     1289\n",
              "machine_learning_techniques        1223\n",
              "deep_learning                      1131\n",
              "knowledge_acquisition              1020\n",
              "regression_analysis                1010\n",
              "artificial_neural_network           925\n",
              "machine_learning_methods            832\n",
              "procedure                           819\n",
              "supervised_learning                 801\n",
              "pattern_recognition                 793\n",
              "computer_simulation                 740\n",
              "male                                728\n",
              "semantics                           684\n",
              "classifier                          682\n",
              "classification_accuracy             637\n",
              "classification                      624\n",
              "clustering_algorithms               603\n",
              "image_processing                    599\n",
              "prediction                          581\n",
              "Name: Keywords, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u62MgUBao-4-"
      },
      "source": [
        "df_filtered = df.groupby('Keywords').filter(lambda x: x['Keywords'].count()>=1532)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKGxEWlrEc1P",
        "outputId": "53aa8b69-953a-49e5-af06-f07acbf02597"
      },
      "source": [
        "len(df_filtered)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30lhlupMbPmp"
      },
      "source": [
        "x = df_filtered['abstract_cleaned']\n",
        "y = df_filtered['Keywords'].tolist()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt27J7i8hnTc",
        "outputId": "46051227-c49b-4c9b-f394-a38631b6f1ad"
      },
      "source": [
        "print(type(x))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHv6TFa3q9ew"
      },
      "source": [
        "### preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idgyTE40q9ex"
      },
      "source": [
        "### import embedding models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6xp7A79q9ex"
      },
      "source": [
        "from gensim.models import KeyedVectors \n",
        "embeddings = KeyedVectors.load_word2vec_format('ft_model_cb.vec', binary=True)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zZI2gGTq9ex"
      },
      "source": [
        "### text to embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8sF13Xqq9ex"
      },
      "source": [
        "#### tokenize and pad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6FCrpc6q9ex"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "max_length = 100\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(x)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "emb_size=embeddings.vector_size\n",
        "\n",
        "encoded_docs = t.texts_to_sequences(x)\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFDYf2_BfbLO",
        "outputId": "1c56f8e0-a965-4822-a52d-46620fbccf14"
      },
      "source": [
        "type(padded_docs)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hPhdnI1q9ex"
      },
      "source": [
        "#### create embedding weights matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHS0kuMHq9ex"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, emb_size))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        if word in embeddings.vocab:\n",
        "            weight_matrix[i] = embedding[word]\n",
        "    return weight_matrix"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPaKRndOq9ex"
      },
      "source": [
        "### create embedding features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulE7YLK3xvqY",
        "outputId": "60e05b5b-6d2b-4081-b683-92c887321d0e"
      },
      "source": [
        "len(list(set(y)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0H2D7Yyq9ex"
      },
      "source": [
        "### LSTM classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxaacf-Dq9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff40ccd4-1360-476d-dbc5-00350427f6f9"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, LSTM, Bidirectional, GRU, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(embeddings, t.word_index)\n",
        "print(embedding_vectors.shape)\n",
        "e = Embedding(vocab_size, emb_size, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "\n",
        "#params\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "n_labels = len(list(set(y)))\n",
        "# create the model\n",
        "def baseline_model(optimizer=opt):\n",
        "    model = Sequential()\n",
        "    model.add(e)\n",
        "    #layer che considera le parole in sequenza 50 neuroni\n",
        "    model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "    model.add(LSTM(50, activation='relu'))\n",
        "    model.add(Dense(n_labels, activation='softmax'))\n",
        "    #loss misura l'errore ed è categorica perchè facciamo classificazione\n",
        "    #in base alla loss la rete cambia iterativamente le feature e sceglie quelle che minimizzano la loss n.iterazioni = epoche\n",
        "    #tante feature fanno decadere l'efficienza\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=baseline_model)\n",
        "# batch_size = [32, 64,128]\n",
        "# epochs = [5, 10, 20]\n",
        "# optimizer = ['Adadelta', 'Nadam']\n",
        "batch_size = [64] #numero di frasi prese alla volta e modificano i pesi \n",
        "epochs = [5]\n",
        "optimizer = ['Adam']#algoritmo che calcola l'errore\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='f1_weighted', verbose=2)\n",
        "print('***CROSS VALIDATED GRID SEARCH***')\n",
        "%time grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54531, 10)\n",
            "***CROSS VALIDATED GRID SEARCH***\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed: 11.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 100, 10)           545310    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 100, 50)           12200     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 9)                 459       \n",
            "=================================================================\n",
            "Total params: 578,169\n",
            "Trainable params: 32,859\n",
            "Non-trainable params: 545,310\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "372/372 [==============================] - 40s 108ms/step - loss: 2.0488 - accuracy: 0.2524\n",
            "Epoch 2/10\n",
            "372/372 [==============================] - 41s 111ms/step - loss: 2.0368 - accuracy: 0.2527\n",
            "Epoch 3/10\n",
            "372/372 [==============================] - 46s 125ms/step - loss: 2.0366 - accuracy: 0.2527\n",
            "Epoch 4/10\n",
            "372/372 [==============================] - 41s 109ms/step - loss: 2.0359 - accuracy: 0.2527\n",
            "Epoch 5/10\n",
            "372/372 [==============================] - 40s 108ms/step - loss: 2.0363 - accuracy: 0.2527\n",
            "Epoch 6/10\n",
            "372/372 [==============================] - 40s 108ms/step - loss: 2.0359 - accuracy: 0.2527\n",
            "Epoch 7/10\n",
            "372/372 [==============================] - 40s 108ms/step - loss: 2.0358 - accuracy: 0.2527\n",
            "Epoch 8/10\n",
            "372/372 [==============================] - 41s 109ms/step - loss: 2.0359 - accuracy: 0.2527\n",
            "Epoch 9/10\n",
            "372/372 [==============================] - 40s 108ms/step - loss: 2.0357 - accuracy: 0.2527\n",
            "Epoch 10/10\n",
            "372/372 [==============================] - 40s 108ms/step - loss: 2.0355 - accuracy: 0.2527\n",
            "CPU times: user 11min 38s, sys: 40.6 s, total: 12min 19s\n",
            "Wall time: 18min 35s\n",
            "Best: 0.101976 using {'batch_size': 64, 'epochs': 10, 'optimizer': 'Adam'}\n",
            "0.101976 (0.004028) with: {'batch_size': 64, 'epochs': 10, 'optimizer': 'Adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RomQjHDfq9ey"
      },
      "source": [
        "### predict test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViS6DfhwfMIB",
        "outputId": "384ae99b-66a2-480e-e424-68c75b205e27"
      },
      "source": [
        "pred"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.17380586, 0.07119283, 0.05620811, 0.04656512, 0.10976699,\n",
              "       0.24659415, 0.16123617, 0.04837783, 0.08625301], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz72PVa4q9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c58346-4576-41f9-caec-55a7f231febb"
      },
      "source": [
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "#results on the test set for the best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred,average='micro')\n",
        "print('***RESULTS ON TEST SET***')\n",
        "print(\"accuracy_score\", accuracy)\n",
        "print(\"f1_score\", f1)\n",
        "print('\\n')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#macro la media della f1 tra una classe e l'altra"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "***RESULTS ON TEST SET***\n",
            "accuracy_score 0.25267287886218737\n",
            "f1_score 0.25267287886218737\n",
            "\n",
            "\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "        artificial_intelligence       0.00      0.00      0.00      1762\n",
            "classification_(of_information)       0.00      0.00      0.00       754\n",
            "                    data_mining       0.00      0.00      0.00       578\n",
            "             feature_extraction       0.00      0.00      0.00       460\n",
            "            learning_algorithms       0.00      0.00      0.00      1175\n",
            "               learning_systems       0.25      1.00      0.40      2576\n",
            "               machine_learning       0.00      0.00      0.00      1547\n",
            "                neural_networks       0.00      0.00      0.00       487\n",
            "        support_vector_machines       0.00      0.00      0.00       856\n",
            "\n",
            "                       accuracy                           0.25     10195\n",
            "                      macro avg       0.03      0.11      0.04     10195\n",
            "                   weighted avg       0.06      0.25      0.10     10195\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTcaoU6ApeAD",
        "outputId": "4378b48b-20b3-4ba2-850a-27623acfa097"
      },
      "source": [
        "probability_class_1 = best_model.predict_proba(x_test[0])[:, 1]\n",
        "probability_class_2 = best_model.predict_proba(x_test[0])[:, 2]\n",
        "probability_class_3 = best_model.predict_proba(x_test[0])[:, 3]\n",
        "probability_class_4 = best_model.predict_proba(x_test[0])[:, 4]\n",
        "probability_class_5 = best_model.predict_proba(x_test[0])[:, 5]\n",
        "probability_class_6 = best_model.predict_proba(x_test[0])[:, 6]\n",
        "probability_class_7 = best_model.predict_proba(x_test[0])[:, 7]\n",
        "probability_class_8 = best_model.predict_proba(x_test[0])[:, 8]\n",
        "\n",
        "probability_class_1"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_3_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.1022577 , 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769,\n",
              "       0.10225769, 0.10225769, 0.10225769, 0.10225769, 0.10225769],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTb05tnnq751",
        "outputId": "a1654f1f-d01d-4ca8-af1f-238f65f32f78"
      },
      "source": [
        "probability_class_2"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363224, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222,\n",
              "       0.09363222, 0.09363222, 0.09363222, 0.09363222, 0.09363222],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVmRxYtXq9V4",
        "outputId": "3b13791b-7e2e-4ded-9dd5-2bb8b7245de7"
      },
      "source": [
        "probability_class_3"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996257,\n",
              "       0.08996256, 0.08996257, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256,\n",
              "       0.08996256, 0.08996256, 0.08996256, 0.08996256, 0.08996256],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2ROsdlOq-Xl",
        "outputId": "bd9549e2-41b6-4e30-d3c4-fda89a954a7c"
      },
      "source": [
        "probability_class_4"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670273,\n",
              "       0.11670272, 0.11670274, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272,\n",
              "       0.11670272, 0.11670272, 0.11670272, 0.11670272, 0.11670272],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITCzF5Skq9ey"
      },
      "source": [
        "### More results with more resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFwGBPHBq9ey"
      },
      "source": [
        "<img src='img/48workers.PNG'>"
      ]
    }
  ]
}